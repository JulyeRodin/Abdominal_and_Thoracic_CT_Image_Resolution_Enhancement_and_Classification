{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE/H+vF+0sQSrjXsEvA/mU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/211iey/DScover-main-project-D-/blob/main/GAN_CIRCLE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRKYSoIPrNbt",
        "outputId": "0026214d-e9b6-448c-a4d2-302348def0d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow keras numpy opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yq7epGmrSJQ",
        "outputId": "df9d27ff-6845-4513-dbc0-13d1b98a0697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, LeakyReLU, Activation, Add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "wPeJ552brdjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 로드"
      ],
      "metadata": {
        "id": "69z-kR4rl_D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "giW2rRanrrjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldct_to_fdct_map = {\n",
        "    \"adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib\": \"adenocarcinoma\",\n",
        "    \"squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa\": \"squamous.cell.carcinoma\",\n",
        "    \"large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa\": \"large.cell.carcinoma\",\n",
        "    \"normal\": \"normal\"\n",
        "}"
      ],
      "metadata": {
        "id": "cxdvfjgTnZDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(base_dir, target_size=(512, 512)):\n",
        "    data = []\n",
        "    for folder in os.listdir(base_dir):\n",
        "        class_dir = os.path.join(base_dir, folder)\n",
        "        for filename in os.listdir(class_dir):\n",
        "            if filename.endswith('.png') or filename.endswith('.jpg'):\n",
        "                img_path = os.path.join(class_dir, filename)\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                img = cv2.resize(img, target_size) / 255.0  # 정규화\n",
        "                data.append(img)\n",
        "    # numpy 배열로 변환 후 반환\n",
        "    return np.array(data).reshape(-1, target_size[0], target_size[1], 1)"
      ],
      "metadata": {
        "id": "_jBK-HI1mBpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fdct 이미지 로드"
      ],
      "metadata": {
        "id": "4BacU7jMntuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdct_train = load_images('/content/drive/MyDrive/DScover_main_project/fdct_data512/train')\n",
        "fdct_valid = load_images('/content/drive/MyDrive/DScover_main_project/fdct_data512/valid')\n",
        "fdct_test = load_images('/content/drive/MyDrive/DScover_main_project/fdct_data512/test')"
      ],
      "metadata": {
        "id": "N7AKiOvEmHeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldct_train = load_images('/content/drive/MyDrive/DScover_main_project/ldct_data512/train')\n",
        "ldct_valid = load_images('/content/drive/MyDrive/DScover_main_project/ldct_data512/valid')\n",
        "ldct_test = load_images('/content/drive/MyDrive/DScover_main_project/ldct_data512/test')"
      ],
      "metadata": {
        "id": "6_68tejTmgW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ldct_train shape:\", ldct_train.shape)\n",
        "print(\"fdct_train shape:\", fdct_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQE2Z82Spy_D",
        "outputId": "80918a68-73bc-4ef7-8917-51ea22cf7674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ldct_train shape: (613, 512, 512, 1)\n",
            "fdct_train shape: (613, 512, 512, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN-CIRCLE 구현\n",
        "왜냐면 데이터가 unpaired라서.. paired 하려면 할 수 있을 것 같은데"
      ],
      "metadata": {
        "id": "I_BqcdNgoY7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, LeakyReLU\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "IM7eqPXUoep1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generator"
      ],
      "metadata": {
        "id": "RfEi7b7FoqoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(input_shape=(512, 512, 1)):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(64, (7, 7), padding='same', activation='relu')(inputs)\n",
        "    x = Conv2D(128, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "    x = Conv2D(256, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "    x = Conv2DTranspose(128, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "    x = Conv2DTranspose(64, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "    outputs = Conv2D(1, (7, 7), padding='same', activation='tanh')(x)\n",
        "    return Model(inputs, outputs, name=\"Generator\")"
      ],
      "metadata": {
        "id": "_msr1-wAopHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## discriminator"
      ],
      "metadata": {
        "id": "AFn6QnVAovOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(input_shape=(512, 512, 1)):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(64, (4, 4), strides=2, padding='same')(inputs)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(128, (4, 4), strides=2, padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(256, (4, 4), strides=2, padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(512, (4, 4), strides=2, padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    outputs = Conv2D(1, (4, 4), padding='same')(x)\n",
        "    return Model(inputs, outputs, name=\"Discriminator\")"
      ],
      "metadata": {
        "id": "YmaqS9jEouaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 초기화"
      ],
      "metadata": {
        "id": "-s0kk7_qo0Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_g = build_generator()  # LDCT → FDCT\n",
        "generator_f = build_generator()  # FDCT → LDCT\n",
        "discriminator_x = build_discriminator()  # FDCT 판별기\n",
        "discriminator_y = build_discriminator()  # LDCT 판별기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRI2CZ8soyLt",
        "outputId": "076d5d49-adc0-4602-f5da-a4087fa3ac8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_optimizer = Adam(1e-4, beta_1=0.5)\n",
        "disc_optimizer = Adam(1e-4, beta_1=0.5)"
      ],
      "metadata": {
        "id": "1Up4-r2Oo6JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 손실함수\n",
        "1. adversarial loss\n",
        "2. cycle consistency loss\n",
        "3. identity loss"
      ],
      "metadata": {
        "id": "mp66Ct3to_ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adversarial Loss\n",
        "def adversarial_loss(real, fake):\n",
        "    return tf.reduce_mean(tf.square(real - 1)) + tf.reduce_mean(tf.square(fake))"
      ],
      "metadata": {
        "id": "25GDdy2zo70-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cycle Consistency Loss\n",
        "def cycle_consistency_loss(real, cycled):\n",
        "    return tf.reduce_mean(tf.abs(real - cycled))"
      ],
      "metadata": {
        "id": "gfgcevvspHFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identity Loss\n",
        "def identity_loss(real, same):\n",
        "    return tf.reduce_mean(tf.abs(real - same))"
      ],
      "metadata": {
        "id": "kwTUBgYEpIRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 루프"
      ],
      "metadata": {
        "id": "qTQqbse3pK6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "batch_size = 4"
      ],
      "metadata": {
        "id": "zE7KLwUhpJVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "    print(f\"Epoch {epoch+1}/50\")\n",
        "\n",
        "    for i in range(0, len(ldct_train), batch_size):\n",
        "        # 배치 데이터 준비\n",
        "        ldct_batch = ldct_train[i:i+batch_size]\n",
        "        fdct_batch = fdct_train[i:i+batch_size]\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # 생성기 예측\n",
        "            fake_fdct = generator_g(ldct_batch)\n",
        "            fake_ldct = generator_f(fdct_batch)\n",
        "            cycled_ldct = generator_f(fake_fdct)\n",
        "            cycled_fdct = generator_g(fake_ldct)\n",
        "\n",
        "            # 판별기 예측\n",
        "            disc_real_fdct = discriminator_x(fdct_batch)\n",
        "            disc_fake_fdct = discriminator_x(fake_fdct)\n",
        "            disc_real_ldct = discriminator_y(ldct_batch)\n",
        "            disc_fake_ldct = discriminator_y(fake_ldct)\n",
        "\n",
        "            # 손실 계산\n",
        "            gen_g_loss = adversarial_loss(disc_real_fdct, disc_fake_fdct) \\\n",
        "                         + cycle_consistency_loss(ldct_batch, cycled_ldct) \\\n",
        "                         + identity_loss(fdct_batch, fake_fdct)\n",
        "\n",
        "            gen_f_loss = adversarial_loss(disc_real_ldct, disc_fake_ldct) \\\n",
        "                         + cycle_consistency_loss(fdct_batch, cycled_fdct) \\\n",
        "                         + identity_loss(ldct_batch, fake_ldct)\n",
        "\n",
        "            disc_x_loss = adversarial_loss(disc_real_fdct, disc_fake_fdct)\n",
        "            disc_y_loss = adversarial_loss(disc_real_ldct, disc_fake_ldct)\n",
        "\n",
        "        # 그래디언트 계산\n",
        "        gen_g_gradients = tape.gradient(gen_g_loss, generator_g.trainable_variables)\n",
        "        gen_f_gradients = tape.gradient(gen_f_loss, generator_f.trainable_variables)\n",
        "        disc_x_gradients = tape.gradient(disc_x_loss, discriminator_x.trainable_variables)\n",
        "        disc_y_gradients = tape.gradient(disc_y_loss, discriminator_y.trainable_variables)\n",
        "\n",
        "        # 그래디언트 적용\n",
        "        gen_optimizer.apply_gradients(zip(gen_g_gradients, generator_g.trainable_variables))\n",
        "        gen_optimizer.apply_gradients(zip(gen_f_gradients, generator_f.trainable_variables))\n",
        "        disc_optimizer.apply_gradients(zip(disc_x_gradients, discriminator_x.trainable_variables))\n",
        "        disc_optimizer.apply_gradients(zip(disc_y_gradients, discriminator_y.trainable_variables))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9c4SeatpOPr",
        "outputId": "6c72993a-3e2b-4bdd-fdcf-cdf928fc94de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 성능 확인\n",
        "print(f\"Epoch {epoch+1}, Gen_G_Loss: {gen_g_loss.numpy()}, Gen_F_Loss: {gen_f_loss.numpy()}, \"\n",
        "      f\"Disc_X_Loss: {disc_x_loss.numpy()}, Disc_Y_Loss: {disc_y_loss.numpy()}\")"
      ],
      "metadata": {
        "id": "VBDQ6chZrNwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 평가 w/ PSNR, SSIM"
      ],
      "metadata": {
        "id": "ZCm21FCPq5O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim"
      ],
      "metadata": {
        "id": "maVlgaBbpQZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restored_fdct = generator_g.predict(ldct_test)"
      ],
      "metadata": {
        "id": "3dYQit-kpgIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psnr_scores = [psnr(fdct_test[i], restored_fdct[i]) for i in range(len(fdct_test))]\n",
        "ssim_scores = [ssim(fdct_test[i].squeeze(), restored_fdct[i].squeeze()) for i in range(len(fdct_test))]\n",
        "\n",
        "print(\"Average PSNR:\", np.mean(psnr_scores))\n",
        "print(\"Average SSIM:\", np.mean(ssim_scores))"
      ],
      "metadata": {
        "id": "g_oEPUB4q-Hx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}